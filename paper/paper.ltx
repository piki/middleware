\documentclass{llncs}
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{subfigure}
\usepackage{tikz}
\frontmatter
\pagestyle{headings}
\mainmatter
\title{Peer-to-Peer Keyword Search: A Retrospective}
\titlerunning{Peer-to-Peer Search}
\author{Patrick Reynolds\inst{1} \and Amin Vahdat\inst{2}}
\authorrunning{Reynolds and Vahdat}
\institute{GitHub, Inc.
\and
University of California, San Diego and Google, Inc.}

\date{}
\begin{document}
\maketitle

\begin{abstract}
Peer-to-peer systems have been an exciting area of research.  Challenges in building have included scalability, reliability, security, and---of particular interest to these authors---search functionality.  This paper surveys some of the history of the field, looks at the lasting impacts of peer-to-peer research, and provides at least one view of where we go from here.
\end{abstract}

\section{Introduction}
2001 was an exciting time for research on peer-to-peer systems.  Napster~\cite{napster} had recently been shut down for abetting widespread copyright violation~\cite{napster-dies}.  Gnutella~\cite{gnutella-ripeanu} and Freenet~\cite{freenet-ian,freenet-theo} survived but had a completely unstructured design that compromised performance and search completeness.  Other peer-to-peer systems~\cite{kazaa,morpheus,grokster} built on Napster's ideas or Gnutella's protocol and were eventually shut down~\cite{kazaa-dies,morpheus-dies,grokster-dies} as Napster was.  The situation called both for better protocol design and for applications other than file sharing.

Better protocol design came first.  Chord~\cite{chord}, CAN~\cite{can}, Pastry~\cite{pastry}, Tapestry~\cite{tapestry}, and Kademlia~\cite{kademlia} collectively introduced the idea of structured, decentralized overlay networks.  All featured efficient---$O(lg~n)$ or $O(n^{1/k})$---mapping from an arbitrary key to the node responsible for that key.

Applications came next.  The first was the distributed hash table, or DHT, mapping keys to values stored in the network.  DHTs quickly became synonymous with the overlay networks that implement them.  The Cooperative File System~\cite{cfs} built cached retrieval of file-sized objects on top of the Chord DHT.  At least initially, however, these applications provided no search functionality.

Napster's search functionality was implemented by a central server.  Gnutella's was implemented by broadcasting search terms to every node in the overlay.  In 2002, we set out to design a complete, efficient keyword search service~\cite{reynolds2003} for applications based on DHTs.

This paper revisits our original paper, surveys some of the research on peer-to-peer keyword searching that followed, examines the state of peer-to-peer technologies today, and identifies some of the lasting impacts that peer-to-peer networks have had.

\section{Efficient peer-to-peer searching}

A good peer-to-peer search feature needs to be decentralized, efficient, and complete.  Early peer-to-peer systems were not.  Napster's was centralized, which made it easy to shut down.  Gnutella's was both inefficient and incomplete: it flooded queries throughout the network up to a fixed radius $TTL$, but could not locate any resources more than $TTL$ hops away.  Early DHTs did not provide search at all.

\subsection{Our contribution}

The simplest implementation of keyword search on a DHT is an {\em inverted index}, a shown in Figure~\ref{naive}.  The DHT maps each keyword to a list of all documents containing that keyword.  A client performing a search for one keyword retrieves the list of documents associated with that keyword.  To perform a conjunctive (``and'') query of multiple keywords, the client retrieves the list of documents for each keyword and locally calculates the intersection.  This approach is clearly decentralized and complete, but it is not especially efficient.  If the user searches for keywords that individually appear in many documents but that rarely appear together, then downloading entire word lists is wasteful.

\begin{figure}[t]
\begin{center}
\mbox{\subfigure[\small A simple approach to ``AND''
queries.  Each node stores a list of document IDs corresponding to one
keyword.]{
\scalebox{0.72}{\input{scheme-naive}}
\label{naive}
}\quad\quad\quad
\subfigure[\small Bloom filters help reduce the bandwidth requirement of
``AND'' queries.  The gray box represents the Bloom filter $F(A)$ of the
set $A$.  Note the false positive in the set $B\cap F(A)$ that node
$s_B$ sends back to node $s_A$.]{
\scalebox{0.72}{\input{scheme-bloom}}
\label{bloom}
}}
\end{center}
\caption{Network architecture and protocol overview}
\end{figure}

Our paper proposed three optimizations to this simple approach: Bloom filters, caching, and incremental results.  For these to work, we changed the protocol so that intersections are calculated within the DHT, as shown in Figure~\ref{bloom}.  That is, the client sends the entire query---e.g., ``efficient AND network AND protocols''---to the node hosting the first keyword.  That node sends the remaining words in the query, along with a list of documents containing the first keyword, to the node responsible for the second keyword.  This second node calculates the intersection between the second keyword's document set and the document set it received from the first node.  Forwarding continues in this fashion until all keywords have been considered (all document sets have been intersected), at which point the last node sends the final list back to the client.

{\bf Bloom filters}~\cite{bloom,summary-cache,mullin-1990} are a compact but lossy way to represent membership in a set.  They answer the question ``Is element $x$ in the set,'' occasionally returning ``true'' even when $x$ is not in the set.  In our search system, we used them as a compact way to represent the set of documents that contain a given keyword.  Instead of transferring entire lists of documents matching one or more keywords, the system transferred Bloom filters representing those document lists.

{\bf Caching} reduces both network traffic and latency by avoiding the transfer of information that has been used recently.  Our system cached Bloom-encoded lists of documents matching a given keyword.  Each cache hit eliminated two hops and the associated transfer costs.

{\bf Incremental results} take advantage of the fact that users often only want a few results---say, the best ten---even when many documents match their query.  At each hop, our system transferred only a few Bloom filter-encoded document IDs at a time, rather than the whole list.  This optimization ties the cost of answering a query to the size of the answer the user wants, rather than to the total number of results available.

In addition, our system incorporated the idea of virtual hosts~\cite{cfs}.  Peer-to-peer systems are often heterogeneous in their capabilities: nodes differ in their available CPU power and network capacity.  Assigning more-capable nodes a larger number of virtual hosts let us take advantage of their additional capacity.

Using a corpus of 105,593 HTML documents and a trace of 95,409 web searches, we measured the effectiveness of our optimizations.  For each query, we calculated the number of bytes transferred and the total time for the system to satisfy the query.  Taken together, our optimizations reduced the time to answer a query by about an order of magnitude.

\subsection{WRITEME other paper}

\subsection{WRITEME other paper}

\subsection{WRITEME other paper}

\section{Where we are now}

\subsection{Losses}

\subsection{Advantages}

\subsection{Impacts}

\section{Where we go from here}


FIXME : bibliography styles wildly inconsistent
\bibliography{search}
\bibliographystyle{plain}

\end{document}