\documentclass{llncs}
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{url}
\usepackage{cite}
\frontmatter
\pagestyle{headings}
\mainmatter
\title{Peer-to-Peer Keyword Search: A Retrospective}
\titlerunning{Peer-to-Peer Search}
\author{Patrick Reynolds\inst{1} \and Amin Vahdat\inst{2}}
\authorrunning{Reynolds and Vahdat}
\institute{GitHub, Inc.
\and
University of California, San Diego and Google, Inc.}

\date{}
\begin{document}
\maketitle

\begin{abstract}
Peer-to-peer systems have been an exciting area of research.  Challenges in building them have included scalability, reliability, security, and---of particular interest to these authors---search functionality.  This paper surveys some of the history of the field, looks at the lasting impacts of peer-to-peer research, and provides at least one view of where we go from here.
\end{abstract}

\section{Introduction}
2001 was an exciting time for research on peer-to-peer systems.  Napster~\cite{napster} had recently been shut down for abetting widespread copyright violation~\cite{napster-dies}.  Gnutella~\cite{gnutella-ripeanu} and Freenet~\cite{freenet-ian,freenet-theo} survived but used completely unstructured overlays that compromised performance and search completeness.  Other peer-to-peer systems reused Napster's ideas or Gnutella's protocol, and these too were eventually shut down~\cite{kazaa,kazaa-dies,morpheus-dies,grokster-dies}.  Peer-to-peer systems needed both better protocol design and applications other than file sharing.

Chord~\cite{chord}, CAN~\cite{can}, Pastry~\cite{pastry}, Tapestry~\cite{tapestry}, and Kademlia~\cite{kademlia} collectively introduced the idea of structured, decentralized overlay networks.  The abstraction they implemented was a distributed hash table, or DHT, which mapped fixed-size, opaque keys to arbitrary values.  All of the DHTs were efficient, requiring just $O(\lg n)$ operations to look up a key in an $n$-node system.

Soon after, distributed file systems like the Cooperative File System (CFS)~\cite{cfs} and PAST~\cite{past-hotos} were built on top of DHTs.  At least initially, however, neither the DHTs nor the distributed file systems provided any search functionality.

In 2002, we set out to design a complete, efficient keyword search service~\cite{reynolds2003} for applications based on DHTs.

This paper revisits our original paper, surveys other interesting research on peer-to-peer keyword searching, examines the state of peer-to-peer technologies today, and identifies some of the lasting impacts that peer-to-peer networks have had.

\subsection{A definition}

In 2001 as well as today, the definition of a peer-to-peer system is a fuzzy one.  The most prominent distinguishing characteristic of a peer-to-peer system is that nodes owned by individuals make up the bulk of both the consumers (clients) and providers (servers) of the service.  A peer-to-peer service generally uses bandwidth, storage, and/or CPU time provided by users of the service.  Further, a robust peer-to-peer service should be decentralized enough that no administrator can disable the system.

Some prominent peer-to-peer services, including Napster, Skype prior to 2012, and all BitTorrent search pages, rely on centralized components that can or could be administratively disabled.  We still consider them peer-to-peer services, albeit ones with room for improvement.

\section{Efficient peer-to-peer searching}

A good peer-to-peer search feature needs to be decentralized, efficient, and complete.  Early peer-to-peer systems were not.  Napster's search feature was centralized, which made it unscalable and easy to shut down.  Gnutella's search feature was both inefficient and incomplete: it flooded queries throughout the network up to a fixed number of hops away from the requester, limited by a time-to-live (TTL) value.  However, it could not locate any resources beyond that number of hops.  Early DHTs did not provide search at all.

\subsection{Our contribution}

\begin{figure}[t]
\begin{center}
\mbox{\subfigure[\small A simple approach to ``AND''
queries.  Each node stores a list of document IDs corresponding to one
keyword.]{
\scalebox{0.53}{\input{post-scheme-naive}}
\label{naive}
}\quad\quad\quad
\subfigure[\small Bloom filters help reduce the bandwidth requirement of ``AND'' queries.  $F(A)$ is a Bloom filter representing the set $A$, and $B\cap F(A)$ is the set of all elements from $B$ that matched the Bloom filter $F(A)$.]{
\scalebox{0.53}{\input{post-scheme-bloom2}}
\label{bloom}
}}
\end{center}
\caption{Adding a Bloom filter to DHT search}
\end{figure}

The simplest implementation of keyword search on a DHT uses an {\em inverted index}, as shown in Figure~\ref{naive}.  The DHT maps each keyword to a list of document IDs corresponding to the documents that contain the keyword.  A client performing a search for one keyword retrieves the list of document IDs associated with that keyword.  To perform a conjunctive (``and'') query of multiple keywords, the client retrieves the list of document IDs for each keyword and locally calculates the intersection.  This approach is clearly decentralized and complete, but it is not especially efficient.  If the user searches for keywords that individually appear in many documents but that rarely appear together, then downloading the entire list of document IDs for each word is wasteful.

\begin{figure}[t]
\begin{center}
\mbox{\subfigure[\small $F(A)$ is already in the cache at node $s_B$.]{
\scalebox{0.53}{\input{post-scheme-caching}}
\label{caching}
}\quad\quad\quad
\subfigure[\small Nodes send their data one chunk at a time until the desired intersection size is reached.]{
\scalebox{0.53}{\input{post-scheme-chunks}}
\label{chunks}
}}
\end{center}
\caption{Caching and incremental results}
\end{figure}

Our paper proposed three optimizations to this simple approach: Bloom filters, caching, and incremental results.  For these to work, we changed the protocol from Figure~\ref{naive} so that intersections are calculated within the DHT, as shown in Figure~\ref{bloom}.  In this revised protocol, the client sends the entire query---e.g., ``efficient AND network AND protocols''---to the node hosting the first keyword.  That node sends the remaining words in the query, along with a list of document IDs for the first keyword, to the node responsible for the second keyword.  This second node calculates the intersection between the second keyword's set of document IDs and the set of document IDs it received from the first node.  Forwarding continues in this fashion until all keywords have been considered (all sets of document IDs have been intersected), at which point the last node sends the final list---IDs of the documents containing all the keywords---back to the client.

Bloom filters~\cite{bloom,summary-cache,mullin-1990} are a compact but lossy way to represent membership in a set.  They answer the question ``Is element $x$ in the set,'' occasionally returning false positives---a value of ``true'' even when $x$ is not in the set.  In our search system, we used them as a compact way to represent the set of document IDs for documents that contain a given keyword.  Instead of transferring entire lists of document IDs, the system transferred Bloom filters representing those lists.

Caching reduces both network traffic and latency by avoiding the transfer of information that has been used recently.  Our system cached Bloom-encoded lists of document IDs corresponding to a given keyword.  Figure~\ref{caching} shows an example where node $s_B$ already has the Bloom filter $F(A)$; the client sends the query directly to node $s_B$, eliminating one hop and the cost of transferring $F(A)$.  Each cache hit eliminates one hop and the associated transfer cost.  Caching allowed us to use larger Bloom filters, with a correspondingly lower false-positive rate.

Incremental results take advantage of the fact that users often only want a few results---say, the best ten---even when many documents match their query.  At each hop, our system transferred only a few Bloom filter-encoded document IDs at a time, rather than the whole list.  This optimization tied the cost of answering a query to the size of the answer the user wanted, rather than to the total number of results available.  Figure~\ref{chunks} shows an example in which three chunks of the document list $A$ are sent, and then the requested result size is reached.

In addition, our system incorporated the idea of virtual hosts~\cite{cfs}.  Peer-to-peer systems are often heterogeneous in their capabilities: nodes differ in their available CPU power and network capacity.  Assigning more-capable nodes a larger number of virtual hosts allowed us to take advantage of their additional capacity.

We measured the effectiveness of our optimizations using a corpus of 105,593 HTML documents and a trace of 95,409 web searches.  For each query, we calculated the number of bytes transferred and the total time for the system to satisfy the query.  Taken together, our optimizations reduced the time to answer a query by about an order of magnitude.

\subsection{Similar work}

Other research projects tackled the problem of peer-to-peer keyword search differently.  This section explores three of those systems.

\subsubsection{PIER.}

PIER~\cite{pier} implements relational queries on top of a DHT, scalable to at least thousands of nodes.  Relations composed of tuples are stored in the DHT; each tuple is stored according to its namespace and primary key.  Joins are performed by retrieving the relevant relations with multicast queries, then storing them back in the DHT in temporary relations keyed (in the DHT) by the appropriate column for the join.  Joins based on Bloom filters are provided, as well, to reduce the number of tuples that must be transferred to temporary tables.

PIER provides expressiveness well beyond keyword-index systems, and it can easily be used to implement keyword queries.  However, we believe that the number of nodes involved in each query and the number of bytes transferred among those nodes will be much higher than in a purpose-built keyword-search system.

\subsubsection{PlanetP.}

PlanetP~\cite{planetp} is a file sharing system built with an emphasis on searchability.  Each node in the system hosts the documents its user wishes to share, as opposed to a DHT where those documents would be copied onto unrelated nodes.  Each node builds an inverted index of the keywords in the documents it shares, then computes a Bloom filter to represent the set of keywords found in at least one document on that node.  All nodes in the system flood their Bloom filters to all other nodes, then gossip updates as the Bloom filters change.  When a user wants to perform a query, PlanetP uses the Bloom filters to figure out which remote nodes to contact.  The false-positive aspect of Bloom filters adds an essentially harmless probability that some nodes will be contacted and return zero results.

\subsubsection{OverCite.}

OverCite~\cite{overcite} is a distributed version of CiteSeer, which is a library and search engine for scientific research papers.  OverCite distributes responsibility for storing, indexing, and searching papers among all participating nodes using a DHT.  The responsibility for indexing those documents is divided among $k$ partitions, each of which maps into the DHT.  The rationale for using partitions is to avoid making every search query a broadcast.  Each partition is $1/k$ the size of the full index and is replicated $n/k$ times, so each node must store $1/k$ of the total index and will receive $1/n$ of the query load.  Two additional optimizations are proposed: replicating author and title metadata to all nodes, and replicating common search terms on all nodes.  Both optimizations allow certain queries to be answered by a single node, rather than sending each query to $k$ nodes to cover all partitions.

%%\subsection{InfoBeacons}
%%To be written~\cite{infobeacons}.
%%Not very interesting.
%%InfoBeacons = "beacons" provide meta-search for deep-web search services
%%("sources").  Gradually learns which sources are best for which sorts of
%%queries.  Uses a p2p network of beacons.  User sends a query to one
%%beacon, which exhausts its sources trying to provide enough results before
%%forwarding the query to randomly selected other beacons in the (presumably
%%unstructured) p2p network.

%%\subsection{YaCy}
%%To be written~\cite{yacy}.
%%\subsection{FAROO}
%%To be written~\cite{faroo}.

\section{Where we are now}

Fifteen years have passed since Napster was released, and ten years have passed since the first peer-to-peer search papers were published.  Where are we now?

Very few prominent Internet services or businesses use peer-to-peer technology.  The most successful peer-to-peer system by far is file sharing, accounting for 10\% to 20\% of traffic during peak hours, on fixed (not mobile) networks~\cite{sandvine}.  The only other peer-to-peer system that is a household name in the U.S.---clearly a subjective distinction on our part---is Spotify.  Skype used to rely on a peer-to-peer overlay but no longer does.  Peer-to-peer networks are also popular for live and on-demand video streaming services in China.

Spotify's network is a hybrid of client-server and peer-to-peer protocols~\cite{spotify}.  Each client keeps a persistent connection open to a Spotify server, through which it can browse available content, learn about other clients currently online, and receive the first fifteen seconds of any song where low latency is required.  Desktop (not mobile) clients retrieve full songs directly from other clients whenever possible.  Clients form an unstructured overlay network and use flooding queries with a TTL of two to search for songs.

Spotify's central servers make the peer-to-peer protocol simpler by providing lists of online clients and by providing a backstop data source for content not present within two overlay hops.  The peer-to-peer network offloads the majority of song download traffic.

Skype originally used peer-to-peer technology to provide a user directory and NAT traversal~\cite{skype-p2p}.  Audio and video streams and chat messages go directly from one user to another whenever possible.  Each {\em supernode} maintains a list of logged-in users and a current IP address for each one.  Supernodes also assist in routing calls and chat messages to clients whose device is suspended or behind a firewall.  In 2012, Skype disabled supernode functionality on end-user PCs and created dedicated {\em mega-supernodes} in data centers run by Microsoft~\cite{skype-no-p2p,skype-blog}.

Two large, live-video streaming services in China, PPS.tv (PPStream) and Funshion, use peer-to-peer technology.  PPStream uses the DONet protocol, which is an unstructured, mesh-style multicast~\cite{ppstream,donet}.  Funshion is based on BitTorrent~\cite{funshion-bt}.

%%WRITEME: others that have abandoned p2p?  backup apps?  streaming apps?

\subsection{Disadvantages of peer-to-peer systems}

From the perspective of running an Internet service, peer-to-peer systems couple some desirable properties with some serious challenges.  The most appealing property of a peer-to-peer system is that it lets a business use customers' bandwidth and computing resources without paying for them.  However, in most cases, the challenges overwhelm this potential cost savings:

\begin{itemize}
\item Most last-mile connections have asynchronous bandwidth, heavily favoring downloads over uploads.  A bandwidth-limited peer-to-peer service like file sharing or video streaming must therefore find many uploaders for each downloader.
\item End-user network connections, especially when geographically distant from each other, have roughly 1,000 times lower bandwidth and 1,000 times higher latency than connections within a data center.
\item Using customers' computers in unexpected ways can lead to bad publicity.
\item Peer-to-peer services require users to download and install software, which providers must write and maintain for each target platform.  An unmodified web browser can access centralized services, but it cannot access peer-to-peer services.
\item Customers turn off their computers more often than data centers do.
\item Mobile devices, which account for a rapidly increasing fraction of Internet traffic~\cite{sandvine}, magnify all of these issues: they are battery- and CPU-constrained, their bandwidth is usually slower and often metered, and they cannot run the same client software as desktop PCs.
\item Customer-owned nodes are not trustworthy.
\item Services with data retention or wiretap requirements will likely find it easier to comply with the law if infrastructure is centralized~\cite{skype-calea}.
\end{itemize}

Some of the limitations of customer-owned computing resources, particularly security and reliability, can be overcome with software, at a cost of additional redundancy and complexity.  Others, including poor network connectivity and customers' aversion to installing additional software, are more stubborn.  Overall, the risks and costs of harnessing customer-owned resources are almost always higher than the risks and costs of running services in professionally managed data centers.

Further, hosted computing, storage, and bandwidth resources have gotten dramatically cheaper per unit in the last decade~\cite{hosting-costs}.  Utility computing, both infrastructure as a service (IaaS) and platform as a service (PaaS), allows new Internet services to start out cheaply with just a fraction of a single server, without relying on peer-to-peer systems.  Both Amazon and Google offer a resource-limited tier of hosting services for free.  At this point, even cash-strapped startups favor starting new Internet services in data centers.

Web search in particular is a service that favors data centers over peer-to-peer systems.  Our test corpus contained 105,593 documents, while Google's contains around fifty billion.  A fully featured web search service does things like spelling suggestions, autocomplete, instant search, location awareness, and personalization that are not amenable to caching or representation in Bloom filters.  Our and others' search protocols mask wide-area bandwidth and latency constraints well enough for the demands of a file-sharing service, perhaps, but not well enough to be competitive with a modern search engine built with dedicated computing resources.

\subsection{Advantages of peer-to-peer systems}

In spite of the challenges, the most popular file-sharing services still use peer-to-peer systems~\cite{sandvine,bittorrent,edonkey,ares}.  We believe that this fact is due almost entirely to censorship resistance.  No matter how widely replicated an infrastructure-based service is, it is still vulnerable to a well placed letter, phone call, domain seizure, or DMCA takedown notice.  Peer-to-peer systems have proven far more resilient.  In 1993, John Gilmore said, ``The Net interprets censorship as damage and routes around it''~\cite{gilmore-time}.  Peer-to-peer systems codify that ideal in software.

Much of the censorship exercised against web sites and peer-to-peer systems is copyright related.  Publishers of movies, songs, books, and software, among others, hope to preserve their ability to charge for each copy made of their copyrighted works.  However, censorship happens for other reasons, too.  For example:

\begin{itemize}
\item {\bf Political} - In 2008, both the McCain~\cite{mccain-dmca} and Obama~\cite{obama-dmca} campaigns had political messages removed due to DMCA takedown notices.  Also in 2008, during the South Ossetia War, the nation of Georgia blocked all websites with addresses ending in {\tt .ru}~\cite{georgia-russia}.
\item {\bf Moral} - Russia has instituted an unpublished blacklist of sites relating to drugs, suicide, or pornography~\cite{russia-bbc}.  The United Kingdom is creating its own list, currently optional but enabled by default, to restrict access to pornography; web forums; information about violence, terrorism, and eating disorders; and ``esoteric material''~\cite{uk-blacklist}.
\item {\bf Security} - Volkswagen successfully sued to censor research about vulnerabilities in its keyless entry systems~\cite{vw-jalopnik}.  Life science researchers have instituted policies for censoring themselves when research seems to pose more risk than social good~\cite{who-bulletin}.\item {\bf Competitive} - Universal Music Group briefly had Megaupload's ``Mega Song'' removed from YouTube, despite not having any copyright claim against it~\cite{mega-song}.
\item {\bf Suppressing criticism} - KTVU used DMCA takedown requests to remove copies of a newscast in which its anchor read obviously fake and offensive names for the pilots of Asiana flight 214~\cite{asiana}.
\item {\bf Accidental} - The U.S. Immigration and Customs Enforcement (ICE) accidentally took down the hip-hop music blog \url{dajaz1.com} for a year before returning it without explanation~\cite{dajaz1}.  YouTube's Content ID system automatically flags videos for possible removal, but it often flags videos that do not contain copyrighted content or that might qualify as fair use~\cite{content-id}.  While attempting to prevent unauthorized distribution of Windows~8 and Office, Microsoft has accidentally requested that Google remove links to the BBC, Wikipedia, and OpenOffice, among others~\cite{microsoft-win8,microsoft-office}.
\end{itemize}
In each case, censorship was possible because a small number of administrative entities---ISPs, domain registrars, YouTube, research conference organizers, etc.---could be compelled to block or remove the content.  Peer-to-peer systems provide an alternative distribution channel for content when censors get too heavy handed.

Formal peer-to-peer systems are not the only way that the Internet routes around censorship.  Content that is small and not obviously offensive or copyright-infringing will often end up widely distributed and widely mirrored if someone tries to censor it.  Far more people know what Barbara Streisand's house looks like, how badly the Suburban Express bus company treats its customers, and how to 3D print a gun than would have if interested parties had not attempted to censor that information~\cite{streisand,suburban-express,3d-guns-bbc}.  In a sense, technology news sites, parodies, memes, and web forums act like an informal peer-to-peer network when they mirror content in this fashion.

\subsection{Impacts}

Several ideas from peer-to-peer systems have found their way into systems that are not strictly peer-to-peer.  Most large-scale Internet services are geographically distributed, self-organizing, and resilient.  Some systems, including the Cassandra database and the Tahoe-LAFS file system, explicitly incorporate DHTs~\cite{cassandra,tahoe-lafs}.

BOINC---the computing platform that runs SETI@home~\cite{boinc}---runs primarily on end users' computers, much like a peer-to-peer system.  Unlike nodes in a peer-to-peer system, nodes in BOINC are only servers and do not consume the service that other nodes provide.  Also, nodes are centrally managed rather than self-organizing.  However, like a peer-to-peer system, BOINC successfully deals with malicious participants and harnesses spare CPU cycles.

\section{Where we go from here}

We believe that peer-to-peer systems continue to have both technical and social value.  They may be a good way for modestly funded research groups to bootstrap an Internet service.  They provide a stress test for new protocols, because protocols and techniques that work within the resource and security constraints of a peer-to-peer system will often work even better in a centralized system.  Finally, of course, they provide outstanding resistance to censorship, in a way that commercial and centrally managed services cannot.

To that end, we believe the most important focus areas in peer-to-peer research are:
\begin{itemize}
\item {\bf Security} - Services that become popular, or that host content that someone wishes to censor, become the target of attacks.  Attackers may disrupt routing or searching, or they may intercept or modify content.  Peer-to-peer systems have to deal with the possibility that nodes are Byzantine faulty~\cite{byzantine}.
\item {\bf Anonymity} - If the main use case for peer-to-peer systems is distributing censored content, then participants might need to remain anonymous when publishing or retrieving that content.  Providing anonymity in a robust, efficient way could provide immense social value.
\item {\bf Searchability} - Simply put, we cannot read what we cannot find.  BitTorrent users currently rely on centralized, commercial web sites to map keywords to document identifiers.  Eventually, whether through legal changes or technical attacks, these sites will probably get shut down.  Existing and new peer-to-peer search technologies should be applied to ensure that users can find content.
\end{itemize}

In 2013, as in 2001, peer-to-peer networking remains an exciting, fruitful area of research.

\bibliography{search}
\bibliographystyle{plain}
\small

\end{document}